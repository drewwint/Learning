{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Hierarchical Linear Regression Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oriol/bin/miniforge3/envs/intuitive_bayes/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "RANDOM_SEED = 8265\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "np.set_printoptions(2)\n",
    "\n",
    "fish_market = pd.read_csv(\"data/fish-market.csv\")\n",
    "fish_market = fish_market.drop([\"Length2\", \"Length3\"], axis=\"columns\")\n",
    "fish_market[\"log_width\"] = np.log(fish_market.Width)\n",
    "fish_market[\"log_height\"] = np.log(fish_market.Height)\n",
    "fish_market[\"log_length\"] = np.log(fish_market.Length1)\n",
    "fish_market[\"log_weight\"] = np.log(fish_market.Weight)\n",
    "fish_reduced = fish_market[fish_market[\"Weight\"] != 0].copy()\n",
    "\n",
    "fish_test = fish_reduced.sample(frac=0.1, random_state=RANDOM_SEED).sort_index()\n",
    "test_idx = fish_test.index\n",
    "fish_train = fish_reduced.loc[fish_reduced.index.difference(test_idx)]\n",
    "\n",
    "species_idx, species = fish_train.Species.factorize(sort=True)\n",
    "COORDS = {\n",
    "    \"slopes\": [\"width_effect\"],  # , \"height_effect\", \"length_effect\"],\n",
    "    \"species\": species,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Real world examples of hierarchies\n",
    "\n",
    "In the lesson we mentioned that nested and grouped data is very common. Think of 5 real-world examples where hierarchical models would be a good fit. Bonus points if some examples come from data sets you worked with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Explore Distributions Over Parameters\n",
    "\n",
    "In this exercise we want to build stronger intuitions around what the different parameters of the model do. Towards this goal, you will generate plots using different settings and analyzing the outputs.\n",
    "Specifically, try the following settings and describe what changed:\n",
    "* global_mu = -0.5 ; global_sigma = 1 ; local_sigma = 0.1 ; n_local = 8\n",
    "* global_mu = 0.0 ; global_sigma = 0.5 ; local_sigma = 0.5 ; n_local = 8\n",
    "* global_mu = 0.0 ; global_sigma = 0.5 ; local_sigma = 0.1 ; n_local = 16\n",
    "* global_mu = 0.0 ; global_sigma = 10. ; local_sigma = 0.1 ; n_local = 8\n",
    "* global_mu = 0.0 ; global_sigma = 0.001 ; local_sigma = 0.1 ; n_local = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def gen_hier_plot(mu=0, sigma=0.5, eps=0.1, groups=8, seed=2):\n",
    "    \"\"\"\n",
    "    Sample hierarchical data and plot it.\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    from numpy.random import default_rng\n",
    "\n",
    "    rng = default_rng(seed)\n",
    "    \n",
    "    group_dist = stats.norm(mu, sigma)\n",
    "    mus = group_dist.rvs(groups, random_state=rng)\n",
    "    data = stats.norm(mus, eps).rvs(size=(1000, groups), random_state=rng)\n",
    "    x = np.linspace(data.min() - .5, data.max() + .5, 1000)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=2, sharex=True)\n",
    "    pdf = group_dist.pdf(x)\n",
    "    axs[0].plot(x, pdf)\n",
    "    axs[0].set(title=f\"global distribution\\nmu={mu} sigma={sigma}, eps={eps}, groups={groups}\", ylabel=\"Belief\")\n",
    "    axs[1].set(title=\"local distributions\", xlabel=\"x\", ylabel=\"Belief\")\n",
    "    for i in range(groups):\n",
    "        color = sns.color_palette(n_colors = groups)[i]\n",
    "        axs[0].plot(mus[i], 0, \".\", ms=8, color=color)\n",
    "        az.plot_dist(data[:, i], color=color, ax = axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Effect of group size on estimates\n",
    "\n",
    "We saw that for Perch, where we had few data, we get strong shrinkage while for Whitefish, where we had more data, we got less shrinkage. But how do these effects interact? In this exercise, you will explore this relationship more systematically. Towards this goal, you will create a plot where on the x-axis is the number of data points for a species, and on the y-axis is the beta estimate (with error bars reflecting posterior uncertainty) for that species. As we are interested in how strong the shrinkage is towards the global mean include the global beta mu (here you can just take the posterior mean) as a horizontal line (you can use `plt.axhline` for this).\n",
    "\n",
    "It is interesting to compare this to the unpooled model where the shrinkage should be much weaker (and towards 0 instead of towards global mean) so create the same plot for the unpooled model.\n",
    "\n",
    "What do you see in terms of how strong shrinkage is in relation to how much data we have per species? Does this make sense? Describe the effect in your own words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model(coords=COORDS) as unpooled_intercept_unpooled_beta:\n",
    "    # data\n",
    "    log_width = pm.MutableData(\"log_width\", \n",
    "                               fish_train.log_width.values)\n",
    "    log_weight = pm.MutableData(\"log_weight\", \n",
    "                                fish_train.log_weight.values)\n",
    "    species_idx_ = pm.ConstantData(\"species_idx\", species_idx)\n",
    "\n",
    "    # priors\n",
    "    intercept = pm.Normal(\"intercept\", sigma=1.0, dims=\"species\")\n",
    "    β = pm.Normal(\"β\", sigma=0.5, dims=\"species\")\n",
    "    \n",
    "    # linear regression\n",
    "    mu = pm.Deterministic(\"mu\", intercept[species_idx_] + \\\n",
    "                                β[species_idx_] * log_width)\n",
    "\n",
    "    # observational noise\n",
    "    sigma = pm.HalfNormal(\"sigma\", 1.0)\n",
    "\n",
    "    # likelihood\n",
    "    log_obs = pm.Normal(\n",
    "        \"log_obs\",\n",
    "        mu=mu,\n",
    "        sigma=sigma,\n",
    "        observed=log_weight,\n",
    "    )\n",
    "\n",
    "    # sampling\n",
    "    idata_unpooled_intercept_unpooled_beta = pm.sample()\n",
    "\n",
    "\n",
    "with pm.Model(coords=COORDS) as hierarchical_intercept_hierarchical_beta:\n",
    "    # data\n",
    "    log_width = pm.MutableData(\"log_width\", \n",
    "                               fish_train.log_width.values)\n",
    "    log_weight = pm.MutableData(\"log_weight\", \n",
    "                                fish_train.log_weight.values)\n",
    "    species_idx_ = pm.MutableData(\"species_idx\", species_idx)\n",
    "    \n",
    "    # global priors for intercepts\n",
    "    intercept_mu = pm.Normal(\"intercept_mu\", sigma=3.0)\n",
    "    intercept_sigma = pm.HalfNormal(\"intercept_sigma\", sigma=1.0)\n",
    "\n",
    "    # individual intercepts for each species\n",
    "    intercept = pm.Normal(\n",
    "        \"intercept\", mu=intercept_mu, sigma=intercept_sigma, \n",
    "        dims=\"species\"\n",
    "    )\n",
    "    \n",
    "    # global prior for betas\n",
    "    β_mu = pm.Normal(\"β_mu\", sigma=3.0)\n",
    "    β_sigma = pm.HalfNormal(\"β_sigma\", sigma=1.0)\n",
    "\n",
    "    # individual betas for each species\n",
    "    β = pm.Normal(\"β\", mu=β_mu, sigma=β_sigma, dims=\"species\")\n",
    "    \n",
    "    # linear regression\n",
    "    mu = intercept[species_idx_] + β[species_idx_] * log_width\n",
    "\n",
    "    # observational noise\n",
    "    eps = pm.HalfNormal(\"eps\", 1.0)\n",
    "\n",
    "    # likelihood\n",
    "    log_obs = pm.Normal(\n",
    "        \"log_obs\",\n",
    "        mu=mu,\n",
    "        sigma=eps,\n",
    "        observed=log_weight,\n",
    "    )\n",
    "    \n",
    "    # Hit the Inference Button(TM)\n",
    "    idata_hierarchical_intercept_hierarchical_beta = pm.sample(target_accept=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Unpooled, pooled, hierarchical variations\n",
    "\n",
    "Our linear regression model had 2 parameters (per species): an intercept and a beta (slope). In the lesson we put a hierarchy on both of these. However, we can mix and match freely and decide per parameter whether it should be pooled, unpooled, or hierarchical. All of these choices lead to different constraints on the parameters and that's what you'll explore in this exercise. Towards this goal, you will build different variations to get more familiarity with the code. Keep these models and idata objects around, however, as you will analyze the outputs in the next exercise.\n",
    "\n",
    "Build the following versions of the model:\n",
    "* Replace the hierarchy on the intercepts with a pooled style\n",
    "* Replace the hierarchy on the intercepts with an unpooled style\n",
    "* Replace the hierarchy on the slopes with a pooled style\n",
    "* Replace the hierarchy on the slopes with an unpooled style\n",
    "  \n",
    "Run each of these different models and make sure that everything converged well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Effect of Hierarchy\n",
    "\n",
    "Let's examine what type of regression patterns these different models produce. For example, if you fix all slopes to be the same (i.e. pooled), but let the intercepts vary (i.e. hierarchical), how do you think the regression lines would look like?\n",
    "\n",
    "To find out, generate a plot where you plot all the regression lines of the different species in a single plot. If you were to include the uncertainty in this plot it would get too messy, so just the plot regression line for each species of the posterior mean.\n",
    "\n",
    "Now generate this plot for the different variants of the model you created in the previous exercise. What do you observe in each variant, what are the similarities and differences? Do the patterns make sense in terms of how the model is structured? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "def plot_pred(idata, ax, plot_multiple_draws=False, colors=None, title=\"\"):\n",
    "    \"\"\"\n",
    "    Helper function to plot regression lines from the posterior on top of data points.\n",
    "    \"\"\"\n",
    "    x = xr.DataArray(np.linspace(0, 2.5, 150), dims=[\"x_plot\"])\n",
    "    post_mean = idata.posterior.mean((\"chain\", \"draw\"))\n",
    "    y_mu = post_mean[\"intercept\"] + x * post_mean[\"β\"]\n",
    "    \n",
    "    if plot_multiple_draws:\n",
    "        post_subset = az.extract(idata, num_samples=50)\n",
    "        y_reg = post_subset[\"intercept\"] + x * post_subset[\"β\"]\n",
    "\n",
    "    if colors is None:\n",
    "        colors = sns.color_palette(n_colors = len(species))\n",
    "        \n",
    "    for i, species_i in enumerate(species):\n",
    "        fish_spec = fish_reduced[fish_reduced.Species == species_i]\n",
    "        ax.scatter(fish_spec[\"log_width\"], fish_spec[\"log_weight\"], color=colors[i])\n",
    "        ax.plot(x, y_mu.sel(species=species_i), color=colors[i], alpha=0.5, lw=2, label=species_i)\n",
    "\n",
    "        if plot_multiple_draws:\n",
    "            ax.plot(x, y_reg.sel(species=species_i).transpose(..., \"sample\"), color=color, alpha=0.1)\n",
    "\n",
    "    if title != \"\":\n",
    "        ax.set_title(title)\n",
    "\n",
    "    return ax"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "rise": {
   "auto_select": "none",
   "enable_chalkboard": true,
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
